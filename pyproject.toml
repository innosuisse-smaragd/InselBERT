
[project]
name = "llm-model"
version = "0.1"
description = "LLM-based, two-layer clinical fact extraction based on the approach by Steinkamp et al."
authors = [
    {name = "Daniel Reichenpfader", email = "daniel.reichenpfader@bfh.ch"},
]
dependencies = [
    "torch>=2.0.1",
    "transformers>=4.31.0",
    "smaragd-shared-python @ git+https://${GITHUB_ACCESS_TOKEN}@github.com/innosuisse-smaragd/smaragd-shared-python.git@v2.5.0",
    "bentoml[io-json]>=1.0.22",
    "datasets>=2.13.1",
    "seqeval>=1.2.2",
    "matplotlib>=3.7.2",
]
requires-python = ">=3.11"

[tool.pdm.dev-dependencies]
dev = [
    "jupyter>=1.0.0",
    "isort>=5.12.0",
    "pycodestyle>=2.10.0",
    "accelerate>=0.21.0",
    "wandb>=0.15.7",
]

[tool.pdm.scripts] # See: https://pdm.fming.dev/2.7/usage/scripts/
lint = {cmd = "pycodestyle ./src ./test --max-line-length 120"}
# test = {cmd = "pytest"}
check-import-order = {cmd = "isort ./src ./test --check-only"}
fix-import-order = {cmd = "isort ./src ./test"}

build-bento = {cmd = "bentoml build ./src/refinement_model/inference -f ./src/refinement_model/inference/bentofile.yaml"}
run-bento-dev = {cmd = "bentoml serve service:svc --reload --working-dir ./src/refinement_model/inference"}
run-bento = {cmd = "bentoml serve extract_bert:latest"}
containerize-bento = {cmd = "bentoml containerize --platform=linux/amd64 extract_bert:latest"}

pretrain-tokenizer = {call = "src.tokenization.training.exec_training:pretrain_tokenizer"}
evaluate-tokenizer = {call = "src.tokenization.evaluation.exec_evaluation:evaluate_tokenizer"}
further-pretrain = {call = "src.further_pre_training.training.exec_training:further_pretrain_model"}
finetune-model = "python src/fact_extraction_model/training/exec_training.py"
test-inference = "python src/refinement_model/inference/service.py"