{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://lajavaness.medium.com/1-token-classification-vs-span-categorization-52a685e4674a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('/Users/daniel/Projects/Smaragd/sk-llm-01')\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import fact_extraction_model.model.bert_multilabel_classification as model_multilabel\n",
    "from shared.cas_loader import CASLoader\n",
    "from constants import ANNOTATED_REPORTS_PATH\n",
    "\n",
    "BASE_MODEL = \"./serialized_models/medbert_512/\"\n",
    "\n",
    "MAX_LENGTH = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC', 5: 'NCHUNK', 6: 'TIME', 7: 'PLACE'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use method from shared-python\n",
    "tag2id = {'PER': 1, 'ORG': 2, 'LOC': 3, 'MISC': 4, 'NCHUNK': 5, 'TIME': 6, 'PLACE': 7}\n",
    "id2tag = {v:k for k, v in tag2id.items()}\n",
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 3: 'B-ORG',\n",
       " 5: 'B-LOC',\n",
       " 7: 'B-MISC',\n",
       " 9: 'B-NCHUNK',\n",
       " 11: 'B-TIME',\n",
       " 13: 'B-PLACE',\n",
       " 2: 'I-PER',\n",
       " 4: 'I-ORG',\n",
       " 6: 'I-LOC',\n",
       " 8: 'I-MISC',\n",
       " 10: 'I-NCHUNK',\n",
       " 12: 'I-TIME',\n",
       " 14: 'I-PLACE'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'O': 0, \n",
    "    **{f'B-{k}': 2*v - 1 for k, v in tag2id.items()},\n",
    "    **{f'I-{k}': 2*v for k, v in tag2id.items()}\n",
    "}\n",
    "\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "NUM_LABELS = len(id2label)\n",
    "id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/Users/daniel/.cache/huggingface/datasets/json/default-068eea7cbca3671a/0.0.0)\n",
      "Found cached dataset json (/Users/daniel/.cache/huggingface/datasets/json/default-30c2354ba4490ed4/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_json(\"./data/multilabel.train.jsonlines\")\n",
    "val_ds = Dataset.from_json(\"./data/multilabel.validation.jsonlines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loader_cas = CASLoader(ANNOTATED_REPORTS_PATH)\n",
    "#train_dict = loader_cas.load_CAS_convert_to_offset_dict()\n",
    "#train_ds = Dataset.from_pandas(pd.DataFrame(data=train_dict))\n",
    "#train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selon l'ethnologue Maurice Duval, « dire que ce mouvement de la gauche radicale est « une secte », ce n'est pas argumenter légitimement contre ses idées, mais c'est suggérer qu'il est malfaisant, malsain et que sa disparition serait souhaitable ».\n",
      "PER        - Maurice Duval\n",
      "NCHUNK     - l'ethnologue Maurice Duval\n",
      "NCHUNK     - ses idées\n",
      "NCHUNK     - sa disparition\n",
      "NCHUNK     - ce mouvement de la gauche radicale\n",
      "\n",
      "Adolescent, il joue de la basse dans un groupe de surf music, commence à composer et s'intéresse aux œuvres de musique contemporaine de compositeurs comme Charles Ives, Karlheinz Stockhausen, Mauricio Kagel, ou encore John Cage.\n",
      "PER        - Charles Ives\n",
      "PER        - Karlheinz Stockhausen\n",
      "PER        - Mauricio Kagel\n",
      "PER        - John Cage\n",
      "NCHUNK     - un groupe de surf music\n",
      "NCHUNK     - œuvres de musique contemporaine de compositeurs comme Charles Ives, Karlheinz Stockhausen, Mauricio Kagel, ou encore John Cage\n",
      "\n",
      "Metacritic \", qui détermine une moyenne pondérée entre 0 et 100 basée sur les critiques populaires, a donné un score moyen de 50 % pour le film, basé sur 40 critiques.\n",
      "MISC       - Metacritic\n",
      "NCHUNK     - une moyenne pondérée entre 0 et 100\n",
      "NCHUNK     - les critiques populaires\n",
      "NCHUNK     - un score moyen de 50 %\n",
      "NCHUNK     - 40 critiques\n"
     ]
    }
   ],
   "source": [
    "def print_examples(): \n",
    "    for i in range(3):\n",
    "        example = train_ds[i]\n",
    "        print(f\"\\n{example['text']}\")\n",
    "        for tag_item in example[\"tags\"]:\n",
    "            print(tag_item[\"tag\"].ljust(10), \"-\", example[\"text\"][tag_item[\"start\"]: tag_item[\"end\"]])\n",
    "\n",
    "print_examples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_role_in_span(token_start: int, token_end: int, span_start: int, span_end: int):\n",
    "    \"\"\"\n",
    "    Check if the token is inside a span.\n",
    "    Args:\n",
    "      - token_start, token_end: Start and end offset of the token\n",
    "      - span_start, span_end: Start and end of the span\n",
    "    Returns:\n",
    "      - \"B\" if beginning\n",
    "      - \"I\" if inner\n",
    "      - \"O\" if outer\n",
    "      - \"N\" if not valid token (like <SEP>, <CLS>, <UNK>)\n",
    "    \"\"\"\n",
    "    if token_end <= token_start:\n",
    "        return \"N\"\n",
    "    if token_start < span_start or token_end > span_end:\n",
    "        return \"O\"\n",
    "    if token_start > span_start:\n",
    "        return \"I\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "\n",
    "def tokenize_and_adjust_labels(sample):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - sample (dict): {\"id\": \"...\", \"text\": \"...\", \"tags\": [{\"start\": ..., \"end\": ..., \"tag\": ...}, ...]\n",
    "    Returns:\n",
    "        - The tokenized version of `sample` and the labels of each token.\n",
    "    \"\"\"\n",
    "    # Tokenize the text, keep the start and end positions of tokens with `return_offsets_mapping` option\n",
    "    # Use max_length and truncation to ajust the text length\n",
    "    tokenized = tokenizer(sample[\"text\"], \n",
    "                          return_offsets_mapping=True, \n",
    "                          padding=\"max_length\", \n",
    "                          max_length=MAX_LENGTH,\n",
    "                          truncation=True)\n",
    "    \n",
    "    # We are doing a multilabel classification task at each token, we create a list of size len(label2id)=13 \n",
    "    # for the 13 labels\n",
    "    labels = [[0 for _ in label2id.keys()] for _ in range(MAX_LENGTH)]\n",
    "    \n",
    "    # Scan all the tokens and spans, assign 1 to the corresponding label if the token lies at the beginning\n",
    "    # or inside the spans\n",
    "    for (token_start, token_end), token_labels in zip(tokenized[\"offset_mapping\"], labels):\n",
    "        for span in sample[\"tags\"]:\n",
    "            role = get_token_role_in_span(token_start, token_end, span[\"start\"], span[\"end\"])\n",
    "            if role == \"B\":\n",
    "                token_labels[label2id[f\"B-{span['tag']}\"]] = 1\n",
    "            elif role == \"I\":\n",
    "                token_labels[label2id[f\"I-{span['tag']}\"]] = 1\n",
    "    \n",
    "    return {**tokenized, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968acf01229d418a8467e1775eb51d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c629d8f6284427a84c2a94d4d9ef1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/83 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_ds = train_ds.map(tokenize_and_adjust_labels, remove_columns=train_ds.column_names)\n",
    "tokenized_val_ds = val_ds.map(tokenize_and_adjust_labels, remove_columns=val_ds.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  6909,\n",
       "  4668,\n",
       "  80,\n",
       "  11,\n",
       "  13613,\n",
       "  5047,\n",
       "  26907,\n",
       "  13041,\n",
       "  5936,\n",
       "  4684,\n",
       "  10420,\n",
       "  7174,\n",
       "  6143,\n",
       "  16,\n",
       "  109,\n",
       "  8633,\n",
       "  7627,\n",
       "  3528,\n",
       "  71,\n",
       "  3528,\n",
       "  16741,\n",
       "  9574,\n",
       "  6219,\n",
       "  5103,\n",
       "  27365,\n",
       "  75,\n",
       "  5408,\n",
       "  3528,\n",
       "  7330,\n",
       "  9816,\n",
       "  3528,\n",
       "  5148,\n",
       "  3533,\n",
       "  109,\n",
       "  4990,\n",
       "  3528,\n",
       "  18432,\n",
       "  4675,\n",
       "  124,\n",
       "  16,\n",
       "  71,\n",
       "  3528,\n",
       "  82,\n",
       "  11,\n",
       "  5148,\n",
       "  3533,\n",
       "  17654,\n",
       "  5626,\n",
       "  7209,\n",
       "  8591,\n",
       "  3536,\n",
       "  80,\n",
       "  3589,\n",
       "  11030,\n",
       "  6110,\n",
       "  6219,\n",
       "  29521,\n",
       "  4741,\n",
       "  10810,\n",
       "  3526,\n",
       "  77,\n",
       "  3540,\n",
       "  3589,\n",
       "  4672,\n",
       "  16,\n",
       "  5715,\n",
       "  4677,\n",
       "  71,\n",
       "  11,\n",
       "  5148,\n",
       "  3533,\n",
       "  5732,\n",
       "  3541,\n",
       "  3541,\n",
       "  3589,\n",
       "  4741,\n",
       "  3536,\n",
       "  7627,\n",
       "  11,\n",
       "  77,\n",
       "  3522,\n",
       "  5148,\n",
       "  3533,\n",
       "  5622,\n",
       "  23460,\n",
       "  4677,\n",
       "  5008,\n",
       "  16,\n",
       "  5622,\n",
       "  22885,\n",
       "  4665,\n",
       "  5124,\n",
       "  7627,\n",
       "  3528,\n",
       "  25266,\n",
       "  11617,\n",
       "  8609,\n",
       "  7315,\n",
       "  10948,\n",
       "  3527,\n",
       "  4682,\n",
       "  4809,\n",
       "  15914,\n",
       "  3527,\n",
       "  29601,\n",
       "  14464,\n",
       "  124,\n",
       "  18,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'offset_mapping': [[0, 0],\n",
       "  [0, 3],\n",
       "  [3, 5],\n",
       "  [6, 7],\n",
       "  [7, 8],\n",
       "  [8, 11],\n",
       "  [11, 13],\n",
       "  [13, 16],\n",
       "  [16, 18],\n",
       "  [19, 21],\n",
       "  [21, 23],\n",
       "  [23, 26],\n",
       "  [27, 29],\n",
       "  [29, 32],\n",
       "  [32, 33],\n",
       "  [34, 35],\n",
       "  [36, 40],\n",
       "  [41, 43],\n",
       "  [43, 44],\n",
       "  [45, 46],\n",
       "  [46, 47],\n",
       "  [48, 50],\n",
       "  [50, 52],\n",
       "  [52, 57],\n",
       "  [58, 60],\n",
       "  [61, 63],\n",
       "  [64, 65],\n",
       "  [65, 69],\n",
       "  [69, 70],\n",
       "  [71, 75],\n",
       "  [75, 78],\n",
       "  [78, 79],\n",
       "  [80, 82],\n",
       "  [82, 83],\n",
       "  [84, 85],\n",
       "  [86, 88],\n",
       "  [88, 89],\n",
       "  [90, 93],\n",
       "  [93, 95],\n",
       "  [96, 97],\n",
       "  [97, 98],\n",
       "  [99, 100],\n",
       "  [100, 101],\n",
       "  [102, 103],\n",
       "  [103, 104],\n",
       "  [104, 106],\n",
       "  [106, 107],\n",
       "  [108, 111],\n",
       "  [112, 114],\n",
       "  [114, 116],\n",
       "  [116, 121],\n",
       "  [121, 122],\n",
       "  [123, 124],\n",
       "  [124, 125],\n",
       "  [125, 127],\n",
       "  [127, 130],\n",
       "  [130, 135],\n",
       "  [136, 140],\n",
       "  [140, 142],\n",
       "  [143, 145],\n",
       "  [145, 146],\n",
       "  [147, 148],\n",
       "  [148, 149],\n",
       "  [149, 150],\n",
       "  [150, 152],\n",
       "  [152, 153],\n",
       "  [154, 156],\n",
       "  [156, 158],\n",
       "  [159, 160],\n",
       "  [160, 161],\n",
       "  [161, 163],\n",
       "  [163, 164],\n",
       "  [165, 167],\n",
       "  [167, 168],\n",
       "  [168, 169],\n",
       "  [169, 170],\n",
       "  [170, 172],\n",
       "  [172, 173],\n",
       "  [174, 176],\n",
       "  [176, 177],\n",
       "  [177, 178],\n",
       "  [178, 179],\n",
       "  [180, 182],\n",
       "  [182, 183],\n",
       "  [184, 187],\n",
       "  [187, 189],\n",
       "  [189, 191],\n",
       "  [191, 194],\n",
       "  [194, 195],\n",
       "  [196, 199],\n",
       "  [199, 201],\n",
       "  [201, 203],\n",
       "  [204, 206],\n",
       "  [207, 209],\n",
       "  [209, 210],\n",
       "  [211, 213],\n",
       "  [214, 217],\n",
       "  [217, 220],\n",
       "  [220, 225],\n",
       "  [226, 229],\n",
       "  [229, 230],\n",
       "  [230, 232],\n",
       "  [233, 235],\n",
       "  [235, 237],\n",
       "  [237, 238],\n",
       "  [238, 241],\n",
       "  [241, 244],\n",
       "  [245, 246],\n",
       "  [246, 247],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0]],\n",
       " 'labels': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Token---------|--------Labels----------\n",
      " [CLS]                | []\n",
      " Sel                  | []\n",
      " ##on                 | []\n",
      " l                    | ['B-NCHUNK']\n",
      " '                    | ['I-NCHUNK']\n",
      " eth                  | ['I-NCHUNK']\n",
      " ##no                 | ['I-NCHUNK']\n",
      " ##log                | ['I-NCHUNK']\n",
      " ##ue                 | ['I-NCHUNK']\n",
      " Ma                   | ['B-PER', 'I-NCHUNK']\n",
      " ##ur                 | ['I-PER', 'I-NCHUNK']\n",
      " ##ice                | ['I-PER', 'I-NCHUNK']\n",
      " Du                   | ['I-PER', 'I-NCHUNK']\n",
      " ##val                | ['I-PER', 'I-NCHUNK']\n",
      " ,                    | []\n",
      " «                    | []\n",
      " dire                 | []\n",
      " qu                   | []\n",
      " ##e                  | []\n",
      " c                    | ['B-NCHUNK']\n",
      " ##e                  | ['I-NCHUNK']\n",
      " mo                   | ['I-NCHUNK']\n",
      " ##uv                 | ['I-NCHUNK']\n",
      " ##ement              | ['I-NCHUNK']\n",
      " de                   | ['I-NCHUNK']\n",
      " la                   | ['I-NCHUNK']\n",
      " g                    | ['I-NCHUNK']\n",
      " ##auch               | ['I-NCHUNK']\n",
      " ##e                  | ['I-NCHUNK']\n",
      " radi                 | ['I-NCHUNK']\n",
      " ##cal                | ['I-NCHUNK']\n",
      " ##e                  | ['I-NCHUNK']\n",
      " es                   | []\n",
      " ##t                  | []\n",
      " «                    | []\n",
      " un                   | []\n",
      " ##e                  | []\n",
      " sec                  | []\n",
      " ##te                 | []\n",
      " »                    | []\n",
      " ,                    | []\n",
      " c                    | []\n",
      " ##e                  | []\n",
      " n                    | []\n",
      " '                    | []\n",
      " es                   | []\n",
      " ##t                  | []\n",
      " pas                  | []\n",
      " ar                   | []\n",
      " ##gu                 | []\n",
      " ##mente              | []\n",
      " ##r                  | []\n",
      " l                    | []\n",
      " ##é                  | []\n",
      " ##gi                 | []\n",
      " ##tim                | []\n",
      " ##ement              | []\n",
      " cont                 | []\n",
      " ##re                 | []\n",
      " se                   | ['B-NCHUNK']\n",
      " ##s                  | ['I-NCHUNK']\n",
      " i                    | ['I-NCHUNK']\n",
      " ##d                  | ['I-NCHUNK']\n",
      " ##é                  | ['I-NCHUNK']\n",
      " ##es                 | ['I-NCHUNK']\n",
      " ,                    | []\n",
      " ma                   | []\n",
      " ##is                 | []\n",
      " c                    | []\n",
      " '                    | []\n",
      " es                   | []\n",
      " ##t                  | []\n",
      " su                   | []\n",
      " ##g                  | []\n",
      " ##g                  | []\n",
      " ##é                  | []\n",
      " ##re                 | []\n",
      " ##r                  | []\n",
      " qu                   | []\n",
      " '                    | []\n",
      " i                    | []\n",
      " ##l                  | []\n",
      " es                   | []\n",
      " ##t                  | []\n",
      " mal                  | []\n",
      " ##fa                 | []\n",
      " ##is                 | []\n",
      " ##ant                | []\n",
      " ,                    | []\n",
      " mal                  | []\n",
      " ##sa                 | []\n",
      " ##in                 | []\n",
      " et                   | []\n",
      " qu                   | []\n",
      " ##e                  | []\n",
      " sa                   | ['B-NCHUNK']\n",
      " dis                  | ['I-NCHUNK']\n",
      " ##par                | ['I-NCHUNK']\n",
      " ##ition              | ['I-NCHUNK']\n",
      " ser                  | []\n",
      " ##a                  | []\n",
      " ##it                 | []\n",
      " so                   | []\n",
      " ##uh                 | []\n",
      " ##a                  | []\n",
      " ##ita                | []\n",
      " ##ble                | []\n",
      " »                    | []\n",
      " .                    | []\n",
      " [SEP]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n",
      " [PAD]                | []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = tokenized_train_ds[0]\n",
    "\n",
    "print(\"--------Token---------|--------Labels----------\")\n",
    "for token_id, token_labels in zip(sample[\"input_ids\"], sample[\"labels\"]):\n",
    "    # Decode the token_id into text\n",
    "    token_text = tokenizer.decode(token_id)\n",
    "    \n",
    "    # Retrieve all the indices corresponding to the \"1\" at each token, decode them to label name\n",
    "    labels = [id2label[label_index] for label_index, value in enumerate(token_labels) if value==1]\n",
    "    \n",
    "    # Decode those indices into label name\n",
    "    print(f\" {token_text:20} | {labels}\")\n",
    "    \n",
    "    # Finish when we meet the end of sentence.\n",
    "    if token_text == \"</s>\": \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Native pytorch code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1): The input is a tuple. We retrieve the predictions and the true labels. The ground truth labels true_labels is an array of shape (dataset size, number of tokens, number of labels), each item of the array is 0 or 1. The predictions array is of the same shape, each item is a logit returned by the model.\n",
    "(2): We define the same threshold 0 for the logits and assign 1 to any position in the array where the logit is beyond this threshold. (Equivalently, with the logit 0, the sigmoid function returns 0.5 as the probability that the item belongs to the corresponding label).\n",
    "(3): For each label in id2label, we compute the confusion matrix. The output of this line will be an array of n_labels = 15 confusion matrices.\n",
    "(4): We compute the token-based precision, recall and f1 score for the 14 labels (except “O”) and store the “f1” in the dict metrics. If you want to evaluate with entity-based metrics, use seqeval for example. Note that in case we put 0 if it’s a zero-division.\n",
    "(5): We compute the macro f1 score over the 14 labels and append it to metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a: int, b: int):\n",
    "    return a / b if b > 0 else 0\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Customize the `compute_metrics` of `transformers`\n",
    "    Args:\n",
    "        - p (tuple):      2 numpy arrays: predictions and true_labels\n",
    "    Returns:\n",
    "        - metrics (dict): f1 score on \n",
    "    \"\"\"\n",
    "    # (1)\n",
    "    predictions, true_labels = p\n",
    "    \n",
    "    # (2)\n",
    "    predicted_labels = np.where(predictions > 0, np.ones(predictions.shape), np.zeros(predictions.shape))\n",
    "    metrics = {}\n",
    "    \n",
    "    # (3)\n",
    "    cm = multilabel_confusion_matrix(true_labels.reshape(-1, NUM_LABELS), predicted_labels.reshape(-1, NUM_LABELS))\n",
    "    \n",
    "    # (4) \n",
    "    for label_idx, matrix in enumerate(cm):\n",
    "        if label_idx == 0:\n",
    "            continue # We don't care about the label \"O\"\n",
    "        tp, fp, fn = matrix[1, 1], matrix[0, 1], matrix[1, 0]\n",
    "        precision = divide(tp, tp + fp)\n",
    "        recall = divide(tp, tp + fn)\n",
    "        f1 = divide(2 * precision * recall, precision + recall)\n",
    "        metrics[f\"f1_{id2label[label_idx]}\"] = f1\n",
    "        \n",
    "    # (5)\n",
    "    macro_f1 = sum(list(metrics.values())) / (NUM_LABELS - 1)\n",
    "    metrics[\"macro_f1\"] = macro_f1\n",
    "        \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/fine_tune_bert_output_span_cat\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2.5e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps = 100,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    log_level='critical',\n",
    "    seed=12345\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Projects/Smaragd/sk-llm-01/.venv/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcbdc9ad49424fdc88e710a2b1cd1abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m model_multilabel\u001b[39m.\u001b[39mBertForMultiLabelClassification\u001b[39m.\u001b[39mfrom_pretrained(BASE_MODEL, id2label\u001b[39m=\u001b[39mid2label, label2id\u001b[39m=\u001b[39mlabel2id)\n\u001b[1;32m      5\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      6\u001b[0m     model_init\u001b[39m=\u001b[39mmodel_init,\n\u001b[1;32m      7\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     15\u001b[0m trainer\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msave_pretrained(OUTPUT_DIR)\n",
      "File \u001b[0;32m~/Projects/Smaragd/sk-llm-01/.venv/lib/python3.11/site-packages/transformers/trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1538\u001b[0m )\n\u001b[0;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1544\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/Smaragd/sk-llm-01/.venv/lib/python3.11/site-packages/transformers/trainer.py:1814\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1809\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1812\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1813\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1814\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[1;32m   1815\u001b[0m ):\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1818\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    # For reproducibility\n",
    "    return model_multilabel.BertForMultiLabelClassification.from_pretrained(BASE_MODEL, id2label=id2label, label2id=label2id)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_val_ds,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_multilabel.BertForMultiLabelClassification.from_pretrained(OUTPUT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offsets_and_predicted_tags(example: str, model, tokenizer, threshold=0):\n",
    "    \"\"\"\n",
    "    Get prediction of model on example, using tokenizer\n",
    "    Args:\n",
    "      - example (str): The input text\n",
    "      - model: The span categorizer\n",
    "      - tokenizer: The tokenizer\n",
    "      - threshold: The threshold to decide whether the token should belong to the label. Default to 0, which corresponds to probability 0.5.\n",
    "    Returns:\n",
    "      - List of (token, tags, offset) for each token.\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence to retrieve the tokens and offset mappings\n",
    "    raw_encoded_example = tokenizer(example, return_offsets_mapping=True)\n",
    "    encoded_example = tokenizer(example, return_tensors=\"pt\")\n",
    "    \n",
    "    # Call the model. The output LxK-tensor where L is the number of tokens, K is the number of classes\n",
    "    out = model(**encoded_example)[\"logits\"][0]\n",
    "    \n",
    "    # We assign to each token the classes whose logit is positive\n",
    "    predicted_tags = [[i for i, l in enumerate(logit) if l > threshold] for logit in out]\n",
    "    \n",
    "    return [{\"token\": token, \"tags\": tag, \"offset\": offset} for (token, tag, offset) \n",
    "            in zip(tokenizer.batch_decode(raw_encoded_example[\"input_ids\"]), \n",
    "                   predicted_tags, \n",
    "                   raw_encoded_example[\"offset_mapping\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Du coup, la menace des feux de forêt est permanente, après les incendies dévastateurs de juillet dans le sud-ouest de la France, en Espagne, au Portugal ou en Grèce. Un important feu de forêt a éclaté le 24 juillet dans le parc national de la Suisse de Bohême, à la frontière entre la République tchèque et l'Allemagne, où des records de chaleur ont été battus (36,4C). Un millier d'hectares ont déjà été touchés. Lundi, les pompiers espéraient que l'incendie pourrait être maîtrisé en quelques jours.\"\n",
    "for item in get_offsets_and_predicted_tags(example, model, tokenizer):\n",
    "    print(f\"\"\"{item[\"token\"]:15} - {item[\"tags\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tagged_groups(example: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get prediction of model on example, using tokenizer\n",
    "    Returns:\n",
    "    - List of spans under offset format {\"start\": ..., \"end\": ..., \"tag\": ...}, sorted by start, end then tag.\n",
    "    \"\"\"\n",
    "    offsets_and_tags = get_offsets_and_predicted_tags(example, model, tokenizer)\n",
    "    predicted_offsets = {l: [] for l in tag2id}\n",
    "    last_token_tags = []\n",
    "    for item in offsets_and_tags:\n",
    "        (start, end), tags = item[\"offset\"], item[\"tags\"]\n",
    "        \n",
    "        for label_id in tags:\n",
    "            label = id2label[label_id]\n",
    "            tag = label[2:] # \"I-PER\" => \"PER\"\n",
    "            if label.startswith(\"B-\"):\n",
    "                predicted_offsets[tag].append({\"start\": start, \"end\": end})\n",
    "            elif label.startswith(\"I-\"):\n",
    "                # If \"B-\" and \"I-\" both appear in the same tag, ignore as we already processed it\n",
    "                if label2id[f\"B-{tag}\"] in tags:\n",
    "                    continue\n",
    "                \n",
    "                if label_id not in last_token_tags and label2id[f\"B-{tag}\"] not in last_token_tags:\n",
    "                    predicted_offsets[tag].append({\"start\": start, \"end\": end})\n",
    "                else:\n",
    "                    predicted_offsets[tag][-1][\"end\"] = end\n",
    "        \n",
    "        last_token_tags = tags\n",
    "        \n",
    "    flatten_predicted_offsets = [{**v, \"tag\": k, \"text\": example[v[\"start\"]:v[\"end\"]]} \n",
    "                                 for k, v_list in predicted_offsets.items() for v in v_list if v[\"end\"] - v[\"start\"] >= 3]\n",
    "    flatten_predicted_offsets = sorted(flatten_predicted_offsets, \n",
    "                                       key = lambda row: (row[\"start\"], row[\"end\"], row[\"tag\"]))\n",
    "    return flatten_predicted_offsets\n",
    "\n",
    "print(example)\n",
    "get_tagged_groups(example, model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
