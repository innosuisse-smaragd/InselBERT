{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://discuss.pytorch.org/t/a-model-with-multiple-outputs/10440/13\n",
    "\n",
    "def forward(self, x):\n",
    "    # Do your stuff here\n",
    "    ...\n",
    "    x1 = F.log_softmax(x) # class probabilities\n",
    "    x2 = ... # bounding box calculation\n",
    "    return x1, x2\n",
    "\n",
    "out1, out2 = model(data)\n",
    "loss1 = criterion1(out1, target1)\n",
    "loss2 = criterion2(out2, target2)\n",
    "loss = loss1 + loss2\n",
    "loss.backward()\n",
    "\n",
    "\n",
    "def forward(self, main_input, aux_input):\n",
    "    main_input = self.embedding_1(main_input)\n",
    "    main_input = self.lstm(main_input, (self.hidden, self.state))\n",
    "    aux = self.aux_output(main_input)    \n",
    "\n",
    "    aux_input = self.input_layer(aux_input)\n",
    "    x = torch.cat((aux_input, main_input))\n",
    "    x = F.relu(self.dense_1(x))\n",
    "    x = F.relu(self.dense_2(x))\n",
    "    x = F.relu(self.dense_3(x))\n",
    "    x = self.main_output(x)\n",
    "\n",
    "    return x, aux\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "  def __init__(self,checkpoint,num_labels): \n",
    "    super(CustomModel,self).__init__() \n",
    "    self.num_labels = num_labels \n",
    "\n",
    "    #Load Model with given checkpoint and extract its body\n",
    "    self.model = model = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "    self.dropout = nn.Dropout(0.1) \n",
    "    self.classifier = nn.Linear(768,num_labels) # load and initialize weights\n",
    "\n",
    "  def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "    #Extract outputs from the body\n",
    "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #Add custom layers\n",
    "    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state\n",
    "\n",
    "    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses\n",
    "    \n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "      loss_fct = nn.CrossEntropyLoss()\n",
    "      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "    \n",
    "    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
